{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e624f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 16:20:40.639929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-17 16:20:41.206754: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2023-01-17 16:20:41.206798: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2023-01-17 16:20:41.206803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816d712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f5af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pipeline class\n",
    "\n",
    "\n",
    "from InformationExtraction import InformationExtractionPipeline\n",
    "\n",
    "\n",
    "# example spacy extractor function\n",
    "NER = spacy.load(\"en_core_web_lg\")\n",
    "def tag_extraction_from_spacy(sen, model = NER):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = [word.text for word in annotated.ents \n",
    "                       if word.label_=='PERSON' or word.label_=='ORG'or word.label_=='GPE']\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "def tag_extraction_from_LM(sen, lang, model = nlp):\n",
    "    \n",
    "    ner_results = model(sen)\n",
    "    extracted_names = []\n",
    "    for idx in range(len(ner_results)):\n",
    "        if ner_results[idx]['entity'][0] == 'B':\n",
    "            start = ner_results[idx]['start']\n",
    "            end = ner_results[idx]['end']\n",
    "            j = idx+1\n",
    "            while j < len(ner_results):\n",
    "                if ner_results[j]['entity'][0] == 'B':\n",
    "                    break\n",
    "                elif ner_results[j]['entity'][0] == 'I':\n",
    "                    end = ner_results[j]['end']\n",
    "                j+=1\n",
    "            idx = j\n",
    "        \n",
    "            extracted_names.append(sen[start:end].strip())\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "# example extractor function that uses training labels \n",
    "# sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "def tag_extraction_from_tags(sent, lang, sent_to_tag):\n",
    "\n",
    "    tags = sent_to_tag[sent]\n",
    "    \n",
    "    \n",
    "    if lang=='zh':\n",
    "        sentsWithtags = [(s,t) for s,t in zip(sent,tags.split())]\n",
    "    else:\n",
    "        sentsWithtags = [(s,t) for s,t in zip(sent.split(),tags.split())]\n",
    "        \n",
    "        \n",
    "    entity_list = []\n",
    "    for i,item in enumerate(sentsWithtags):\n",
    "        if 'B-' in item[1]:\n",
    "            j = i\n",
    "            entity = []\n",
    "            while j<len(sentsWithtags):\n",
    "                if sentsWithtags[j][1] =='O':\n",
    "                    break\n",
    "                entity.append(sentsWithtags[j][0])\n",
    "                j+=1\n",
    "            i = j\n",
    "             \n",
    "            if lang=='zh':\n",
    "                entity_list.append(\"\".join(entity))\n",
    "            \n",
    "            else:\n",
    "                entity_list.append(\" \".join(entity))\n",
    "            \n",
    "    \n",
    "\n",
    "    return entity_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba5e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(Set, lang):\n",
    "    data = pd.read_csv(f'../Dataset/{Set}.csv')\n",
    "    data = data[data['lang']==lang]\n",
    "    \n",
    "    if lang=='zh':\n",
    "        data['sent'] = data['sent'].apply(lambda x: \"\".join(x.split()))\n",
    "            \n",
    "    with open(f'./Wiki-jsons/wiki-info-{lang}-{Set}.json','r') as file:\n",
    "        wiki = json.load(file)\n",
    "        wiki = list(wiki.keys())\n",
    "\n",
    "    sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "    detectedLM = 0\n",
    "    detectedTags = 0\n",
    "    totalTags = 0\n",
    "    totalLM = 0\n",
    "    irrelevent = 0\n",
    "    common  = 1\n",
    "    for sent in tqdm(data['sent']):\n",
    "\n",
    "\n",
    "            \n",
    "        tags = tag_extraction_from_tags(sent, lang, sent_to_tag = sent_to_tag)\n",
    "        \n",
    "        if lang!='en':\n",
    "            lm = tag_extraction_from_LM(sent,lang)\n",
    "        else:\n",
    "            lm = tag_extraction_from_spacy(sent)\n",
    "\n",
    "        if lang!='zh':\n",
    "            tags = [\"_\".join([token.capitalize() for token in names.strip().split()]) for names in tags]\n",
    "            lm = [\"_\".join([token.capitalize() for token in names.strip().split()]) for names in lm]\n",
    "        \n",
    "        totalTags += len(tags)\n",
    "        totalLM += len(lm)\n",
    "        \n",
    "        for item in tags:\n",
    "            if item in lm and item in wiki:\n",
    "                detectedLM+=1\n",
    "            if item in wiki:\n",
    "                detectedTags+=1\n",
    "            if item in lm:\n",
    "                common += 1 \n",
    "                \n",
    "        for item in lm:\n",
    "            if item not in tags and item in wiki:\n",
    "                irrelevent+=1\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "    return totalLM, detectedLM, irrelevent, totalTags, detectedTags, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3deaed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 9758/9758 [06:29<00:00, 25.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang: zh, Set: train, Total-LM: 13114, found-LM: 2755, Toatal-Tags: 15226, found-tags: 6553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 505/505 [00:39<00:00, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang: zh, Set: dev, Total-LM: 662, found-LM: 141, Toatal-Tags: 772, found-tags: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Sets = ['train', 'dev']\n",
    "data = pd.read_csv(f'../Dataset/train.csv')\n",
    "# langs = data.lang.unique()\n",
    "langs = ['zh']\n",
    "pcts = []\n",
    "for Set in Sets:\n",
    "    \n",
    "    for lang in langs:\n",
    "        \n",
    "\n",
    "        totalLM, detectedLM, irrelevent, totalTags, detectedTags, common = get_info(Set,lang)\n",
    "    \n",
    "        info = [lang, Set, totalTags , detectedTags, common, totalLM, detectedLM, irrelevent]\n",
    "        print(f\"Lang: {lang}, Set: {Set}, Total-LM: {totalLM}, found-LM: {detectedLM}, Toatal-Tags: {totalTags}, found-tags: {detectedTags}\")\n",
    "        pcts.append(info)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87908cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['zh', 'train', 15226, 6553, 4308, 13114, 2755, 4796],\n",
       " ['zh', 'dev', 772, 297, 247, 662, 141, 199]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5272131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(pcts, columns=['Lang','Set','totalTags' , 'detectedTags', 'common', 'totalLM', 'detectedLM', 'irrelevent'])\n",
    "# df.to_csv('info-label.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
